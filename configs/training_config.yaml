model_name: "unsloth/Qwen3-0.6B"
max_seq_length: 5000
output_dir: "./outputs"
optim: "adamw_torch"
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 2
learning_rate: 3.0e-5
warmup_steps: 61
lr_scheduler_type: "cosine"
logging_steps: 10
fp16: true
report_to: "wandb"
run_name: "nizami-1.7B-cot"
dataloader_num_workers: 16
weight_decay: 0.01
dataset_text_field: "text",
gradient_checkpointing: false
