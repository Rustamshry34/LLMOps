output_dir: "./outputs"
optim: "adamw_torch"
max_length: 3000
per_device_train_batch_size: 2
gradient_accumulation_steps: 2
num_train_epochs: 2
learning_rate: 2.0e-5
warmup_steps: 120
lr_scheduler_type: "cosine"
logging_steps: 50
fp16: true
report_to: "wandb"
dataloader_num_workers: 4
weight_decay: 0.01
gradient_checkpointing: true
