output_dir: "./outputs"
optim: "adamw_torch"
max_length: 512
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
num_train_epochs: 2
learning_rate: 3.0e-5
warmup_steps: 25
lr_scheduler_type: "cosine"
logging_steps: 10
fp16: true
report_to: "wandb"
dataloader_num_workers: 4
weight_decay: 0.01
gradient_checkpointing: true
