{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nizami-1.7B CoT Fine-Tune (Unsloth + LoRA)  \n",
    "> GitHub Actions tarafından otomatik trigger edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. Environment & Secrets ==========\n",
    "import os, subprocess, torch\n",
    "HF_TOKEN   = os.getenv(\"HF_TOKEN\")\n",
    "WANDB_KEY  = os.getenv(\"WANDB_API_KEY\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")  # private repo clone için\n",
    "\n",
    "# Kaggle secret’ları notebook’a düşerken otomatik gelir\n",
    "assert HF_TOKEN and WANDB_KEY, \"HF_TOKEN veya WANDB_API_KEY eksik!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. Repo’yu çek ==========\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Rustamshry34/LLMOps.git\"\n",
    "!git clone $REPO_URL repo\n",
    "%cd repo\n",
    "!pip install -r docker/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Train (pipeline.txt ile aynı hiper-parametreler) ==========\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "login(HF_TOKEN)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-0.6B\",\n",
    "    max_seq_length=5000,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=False,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=False\n",
    ")\n",
    "\n",
    "# ----- data -----\n",
    "reasoning_ds = load_dataset(\"moremilk/CoT_Temporal_Reasoning_Dataset\", split=\"train\")\n",
    "non_reasoning_ds = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "\n",
    "def generate_conversation(examples):\n",
    "    problems, meta, answers = examples[\"question\"], examples[\"metadata\"], examples[\"answer\"]\n",
    "    conversations = []\n",
    "    for p, m, a in zip(problems, meta, answers):\n",
    "        resp = f\"<think>{m.get('reasoning', '')}</think>\\n\\n<answer>{a}</answer>\"\n",
    "        conversations.append([\n",
    "            {\"role\": \"user\", \"content\": p},\n",
    "            {\"role\": \"assistant\", \"content\": resp}\n",
    "        ])\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "reasoning_ds = reasoning_ds.map(generate_conversation, batched=True, remove_columns=reasoning_ds.column_names)\n",
    "reasoning_text = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in reasoning_ds[\"conversations\"]]\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "non_ds = standardize_sharegpt(non_reasoning_ds)\n",
    "non_text = tokenizer.apply_chat_template(non_ds[\"conversations\"], tokenize=False)\n",
    "\n",
    "import pandas as pd, datasets\n",
    "data = pd.Series(reasoning_text + non_text.sample(n=0, random_state=2407).tolist())  # %0 non-reasoning\n",
    "data.name = \"text\"\n",
    "combined_ds = datasets.Dataset.from_pandas(pd.DataFrame(data)).shuffle(seed=3407)\n",
    "\n",
    "# ----- training -----\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import wandb\n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"./outputs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=61,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"nizami-1.7B-cot-kaggle\",\n",
    "    max_seq_length=5000,\n",
    "    dataset_text_field=\"text\",\n",
    "    dataloader_num_workers=16,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model=model, tokenizer=tokenizer, args=args, train_dataset=combined_ds)\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./outputs\")\n",
    "tokenizer.save_pretrained(\"./outputs\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. HF Hub’a yükle ==========\n",
    "!python scripts/upload_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6. Kaggle çıktısını GitHub’a bildir (status = complete) ==========\n",
    "print(\"✅ Kaggle eğitimi tamamlandı, model HF Hub’da.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
